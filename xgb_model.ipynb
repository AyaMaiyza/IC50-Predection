{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcda0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n",
      "Test set - Mean Squared Error (MSE): 0.4903\n",
      "Test set - Root Mean Squared Error (RMSE): 0.7002\n",
      "Test set - Mean Absolute Error (MAE): 0.5114\n",
      "Test set - R-squared (R2): 0.6078\n",
      "Training set - Mean Squared Error (MSE): 0.0025\n",
      "Training set - Root Mean Squared Error (RMSE): 0.0497\n",
      "Training set - Mean Absolute Error (MAE): 0.0337\n",
      "Training set - R-squared (R2): 0.9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['X_scaler_filtered1.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load the data\n",
    "input_csv_path = 'compounds_with_predictions2.csv'  # Replace with your actual CSV file path\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Convert MACCS fingerprints from comma-separated string to a list of integers\n",
    "def maccs_to_array(maccs_str):\n",
    "    try:\n",
    "        # Convert the string to a list of integers and discard the first bit (bit 0)\n",
    "        return np.array(list(map(int, maccs_str.split(',')))[1:], dtype=int)\n",
    "    except ValueError:\n",
    "        return np.zeros(166, dtype=int)  # Default to an array of zeros if there's an error\n",
    "\n",
    "# Apply the function to convert the MACCS fingerprint strings\n",
    "df['MACCS_fingerprint'] = df['MACCS_fingerprint'].apply(maccs_to_array)\n",
    "\n",
    "# Convert MACCS fingerprints to a feature matrix\n",
    "X = np.array(df['MACCS_fingerprint'].tolist())\n",
    "\n",
    "# Extract pIC50 values (target variable y)\n",
    "y = df['pIC50'].values\n",
    "\n",
    "# Use all features after discarding the first bit\n",
    "X_filtered = X\n",
    "\n",
    "# Initialize MinMaxScaler for scaling target variable\n",
    "scaler_y = MinMaxScaler()\n",
    "y_transformed = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_transformed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize StandardScaler for feature scaling\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb = XGBRegressor(random_state=42, use_label_encoder=True, eval_metric='rmse')\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),  # Start from a higher number to avoid very low values\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(4, 10),  # Slightly narrower range\n",
    "    'subsample': uniform(0.7, 0.3),\n",
    "    'colsample_bytree': uniform(0.7, 0.3),\n",
    "    'reg_alpha': uniform(0.0, 1.0),  # L1 regularization\n",
    "    'reg_lambda': uniform(0.0, 1.0)  # L2 regularization\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of parameter settings sampled\n",
    "    cv=4,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Perform random search to find the best hyperparameters\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predict transformed pIC50 values for the test set\n",
    "y_pred_transformed = best_model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_transformed.reshape(-1, 1)).flatten()\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "mse_test = mean_squared_error(y_test_original, y_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test_original, y_pred)\n",
    "r2_test = r2_score(y_test_original, y_pred)\n",
    "\n",
    "# Predict transformed pIC50 values for the training set\n",
    "y_train_pred_transformed = best_model.predict(X_train_scaled)\n",
    "y_train_pred = scaler_y.inverse_transform(y_train_pred_transformed.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "mse_train = mean_squared_error(y_train, y_train_pred_transformed)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred_transformed)\n",
    "r2_train = r2_score(y_train, y_train_pred_transformed)\n",
    "\n",
    "# Print evaluation metrics for test and training sets\n",
    "print(f\"Test set - Mean Squared Error (MSE): {mse_test:.4f}\")\n",
    "print(f\"Test set - Root Mean Squared Error (RMSE): {rmse_test:.4f}\")\n",
    "print(f\"Test set - Mean Absolute Error (MAE): {mae_test:.4f}\")\n",
    "print(f\"Test set - R-squared (R2): {r2_test:.4f}\")\n",
    "\n",
    "print(f\"Training set - Mean Squared Error (MSE): {mse_train:.4f}\")\n",
    "print(f\"Training set - Root Mean Squared Error (RMSE): {rmse_train:.4f}\")\n",
    "print(f\"Training set - Mean Absolute Error (MAE): {mae_train:.4f}\")\n",
    "print(f\"Training set - R-squared (R2): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# Save the best model and scalers for future use\n",
    "joblib.dump(best_model, 'xgb_model_filtered1.pkl')\n",
    "joblib.dump(scaler_y, 'y_scaler_filtered1.pkl')\n",
    "joblib.dump(scaler_X, 'X_scaler_filtered1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550077bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES: CC(C)CCC[C@](C)(O)C4CCC3C/2CCC1C[C@@H](O)CC[C@]1(C)C2=C\\C[C@@]34C, Predicted pIC50: 5.1985\n",
      "Predicted IC50: 6.3320 uM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "import joblib\n",
    "\n",
    "# Load the saved XGBoost model and scalers\n",
    "best_model = joblib.load('xgb_model_filtered1.pkl')  # Load XGBoost model\n",
    "scaler_y = joblib.load('y_scaler_filtered1.pkl')  # Load y-scaler\n",
    "scaler_X = joblib.load('X_scaler_filtered1.pkl')  # Load X-scaler\n",
    "\n",
    "# Define a function to convert SMILES to MACCS fingerprints and discard the first bit\n",
    "def smiles_to_maccs(smiles):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    if molecule is None:\n",
    "        return np.zeros(166, dtype=int)  # Return a default array of 166 bits if SMILES is invalid\n",
    "    maccs_fingerprint = MACCSkeys.GenMACCSKeys(molecule)\n",
    "    return np.array(list(maccs_fingerprint)[1:], dtype=int)  # Convert to 166-bit fingerprint (discarding the first bit)\n",
    "\n",
    "# Example list of new SMILES strings\n",
    "smiles_list = [\n",
    "    'CC(C)CCC[C@](C)(O)C4CCC3C/2CCC1C[C@@H](O)CC[C@]1(C)C2=C\\C[C@@]34C'\n",
    "]\n",
    "\n",
    "# Convert SMILES strings to MACCS fingerprints\n",
    "fingerprints = np.array([smiles_to_maccs(smiles) for smiles in smiles_list])\n",
    "\n",
    "# Ensure the selection filter aligns with the training step\n",
    "# std_devs = np.std(fingerprints, axis=0)\n",
    "# sd_cutoff = 0.1\n",
    "# selected_features = std_devs > sd_cutoff  # Use the same cutoff used during training\n",
    "\n",
    "# Filter and scale the new data\n",
    "# fingerprints_filtered = fingerprints[:, selected_features]  # Apply the same filter used during training\n",
    "fingerprints_filtered = fingerprints  # No filtering applied\n",
    "X_new_scaled = scaler_X.transform(fingerprints_filtered)\n",
    "\n",
    "# Predict using the loaded model\n",
    "y_pred_transformed = best_model.predict(X_new_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_transformed.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Print predictions\n",
    "for smiles, prediction in zip(smiles_list, y_pred):\n",
    "    print(f\"SMILES: {smiles}, Predicted pIC50: {prediction:.4f}\")\n",
    "    predicted_IC50 = 10 ** (-prediction) * 1000000\n",
    "    print(f\"Predicted IC50: {predicted_IC50:.4f} uM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df33a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680c03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
